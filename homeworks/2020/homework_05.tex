\documentclass[11pt,nocut]{article}

\usepackage{../../latex_style/packages}
\usepackage{../../latex_style/notations}

\title{\vspace{-2.0cm}%
	Optimization and Computational Linear Algebra for Data Science\\
Homework 5: Orthogonal matrices, eigenvalues and eigenvectors}
%\author{LÃ©o \textsc{Miolane} \ $\cdot$ \ \texttt{leo.miolane@gmail.com}}
\date{\vspace{-1cm}Due on October 11, 2020}
\setcounter{section}{5}


\begin{document}
\maketitle
\input{./preamble_homeworks.tex}

\begin{problem}[2 points]
	Let $S$ be a subspace of $\R^n$ and let $P_S$ be the matrix of the orthogonal projection onto $S$. Let $M = \Id_n - 2 P_S$.
	\begin{enumerate}[label=\normalfont(\textbf{\alph*})]
		\item Show that the matrix $M$ is orthogonal.
		\item Show that if $\lambda \in \R$ is an eigenvalue of $M$, then $\lambda = 1$ or $\lambda=-1$.
	\end{enumerate}
\end{problem}

\vspace{1mm}


\begin{problem}[2 points]
	Let $v \in \R^n$ be a non-zero vector. What are the eigenvalues of the $n \times n$ matrix
	$$
		M = v v^{\sT}
	$$
	and their multiplicities ? (In the expression $vv^{\sT}$ we see $v$ as a matrix with $1$ column and $n$ rows).
\end{problem}

\vspace{1mm}
\begin{problem}[2 points]
	Let $A \in \R^{n \times n}$ be a symmetric matrix.
	\\
	Show that if $v_1, v_2 \in \R^n$ are two eigenvectors of $A$ associated to distinct eigenvalues $\lambda_1 \neq \lambda_2$ ($Av_1 = \lambda_1 v_1$ and $Av_2 = \lambda_2 v_2$), then $v_1 \perp v_2$.
\end{problem}



\vspace{1mm}

%\begin{problem}[1 points]
	%Let $A \in \R^{n \times n}$ be a matrix that admits $k$ distinct eigenvalues $\lambda_1, \dots, \lambda_k$. For each $i \in \{1, \dots, k\}$ we denote by $m_i$ the multiplicity of the eigenvalues $\lambda_i$. Show that
	%$$
		%m_1 + \cdots + m_k \leq n.
	%$$
%\end{problem}

%\vspace{1mm}

\begin{problem}[4 points]
	Let $A \in \R^{n \times n}$. We assume that there exists a basis $(v_1, \dots, v_n)$ of $\R^n$ consisting of eigenvectors of $A$:
	$$
	A v_i = \lambda_i v_i
	$$
	for all $i \in \{1, \dots, n\}$.
	We assume that 
	$$
	\lambda_1 > |\lambda_i| \qquad \text{for all} \ i \in \{2, \dots, n\}.
	$$
	We consider the following algorithm:
	\begin{itemize}
		\item Initialize $x_0 \in \R^n$.
		\item Perform the updates: $x_{t+1} = \frac{A x_t}{\|A x_t\|}$.
	\end{itemize}
	\begin{enumerate}[label=\normalfont(\textbf{\alph*})]
		\item Show that for all $t \geq 1$, 
			$$
			x_t = \frac{A^t x_0}{\|A^t x_0\|}.
			$$
		\item Assume that $x_0$ is a unit vector ($\|x_0\|=1$) whose direction is chosen uniformly at random (this basically means that all the possible directions for $x_0$ are equally likely to be chosen). Let $(\alpha_1, \dots, \alpha_n)$ be the coordinates of $x_0$ in the basis $(v_1, \dots, v_n)$. Explain we have $\alpha_1 \neq 0$ with probability $1$. You do not have to do a rigorous proof of that, just give an intuitive argument.
		\item Show that
			$$
			x_t \xrightarrow[t \to \infty]{} \frac{\alpha_1 v_1}{\|\alpha_1 v_1\|}
			\qquad \text{and} \qquad
			\|A x_t\| \xrightarrow[t \to \infty]{} \lambda_1.
			$$
	\end{enumerate}
\end{problem}

\vspace{1mm}


\begin{problem}[$\star$]
	Let $A \in \R^{n \times n}$ be a symmetric matrix. We define the function
	$$
	\begin{array}{cccc}
		f:& \R^n \setminus \{0\} & \to & \R \\
		  & x & \mapsto & \displaystyle \frac{x^{\sT} A x}{x^{\sT} x}.
	\end{array}
	$$
	We admit that $f$ has a maximum at some $x_{\star} \in \R^n \setminus \{0\}$.
	Show that $x_{\star}$ verifies
	$$
	A x_{\star} = \lambda x_{\star}, \qquad \text{where} \quad \lambda = f(x_{\star}).
	$$
\end{problem}
\vspace{1cm}
\centerline{\pgfornament[width=7cm]{87}}

%\bibliographystyle{plain}
%\bibliography{./references.bib}
\end{document}
