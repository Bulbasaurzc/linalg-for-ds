\documentclass[11pt,nocut]{article}

\usepackage{../../latex_style/packages}
\usepackage{../../latex_style/notations}

\title{\vspace{-2.0cm}%
	Optimization and Computational Linear Algebra for Data Science\\
Homework 5: Orthogonal matrices, eigenvalues and eigenvectors}
%\author{LÃ©o \textsc{Miolane} \ $\cdot$ \ \texttt{leo.miolane@gmail.com}}
\date{\vspace{-1cm}Due on October 8, 2019}
\setcounter{section}{5}

\begin{document}
\maketitle
\input{./preamble_homeworks.tex}

\begin{problem}[2 points]
	Let $A \in \R^{n \times n}$ be a symmetric matrix.
	\\
	Show that if $v_1, v_2 \in \R^n$ are two eigenvectors of $A$ associated to some eigenvalues $\lambda_1 \neq \lambda_2$ ($Av_1 = \lambda_1 v_1$ and $Av_2 = \lambda_2 v_2$), then $v_1 \perp v_2$.
\end{problem}

\vspace{1mm}

\begin{problem}[3 points]
	Let $S$ be a subspace of $\R^n$ and let $P_S$ be the matrix of the orthogonal projection onto $S$. Let $M = \Id_n - 2 P_S$.
	\begin{enumerate}[label=\normalfont(\textbf{\alph*})]
		\item Show that the matrix $M$ is orthogonal.
		\item Show that if $\lambda \in \R$ is an eigenvalue of $M$, then $\lambda = 1$ or $\lambda=-1$.
	\end{enumerate}
\end{problem}

\vspace{1mm}

%\begin{problem}[1 points]
	%Let $A \in \R^{n \times n}$ be a matrix that admits $k$ distinct eigenvalues $\lambda_1, \dots, \lambda_k$. For each $i \in \{1, \dots, k\}$ we denote by $m_i$ the multiplicity of the eigenvalues $\lambda_i$. Show that
	%$$
		%m_1 + \cdots + m_k \leq n.
	%$$
%\end{problem}

%\vspace{1mm}

\begin{problem}[5 points]
	Let $A \in \R^{n \times n}$. We assume that there exists a basis $(v_1, \dots, v_n)$ of $\R^n$ consisting of eigenvectors of $A$:
	$$
	A v_i = \lambda_i v_i
	$$
	for all $i \in \{1, \dots, n\}$.
	We assume that 
	$$
	\lambda_1 > |\lambda_i| \qquad \text{for all} \ i \in \{2, \dots, n\}.
	$$
	We consider the following algorithm:
	\begin{itemize}
		\item Initialize $x_0 \in \R^n$.
		\item Perform the updates: $x_{t+1} = \frac{A x_t}{\|A x_t\|}$.
	\end{itemize}
	\begin{enumerate}[label=\normalfont(\textbf{\alph*})]
		\item Show that for all $t \geq 1$, 
			$$
			x_t = \frac{A^t x_0}{\|A^t x_0\|}.
			$$
		\item Assume that $x_0$ is a unit vector ($\|x_0\|=1$) whose direction is chosen uniformly at random (this basically means that all the possible directions for $x_0$ are equally likely to be chosen). Let $(\alpha_1, \dots, \alpha_n)$ be the coordinates of $x_0$ in the basis $(v_1, \dots, v_n)$. Explain why we can be sure that $\alpha_1 \neq 0$. You do not have to do a rigorous proof of that, just give an intuitive argument.
		\item Show that
			$$
			x_t \xrightarrow[t \to \infty]{} \frac{\alpha_1 v_1}{\|\alpha_1 v_1\|}
			\qquad \text{and} \qquad
			\|A x_t\| \xrightarrow[t \to \infty]{} \lambda_1.
			$$
		\item Code the algorithm in \texttt{Python}: write a function \texttt{power\_iteration} that takes a matrix $A$ as input and outputs $x_t$ and $\|Ax_t\|$ for the first $t$ such that $\|x_{t}-x_{t-1}\| \leq 10^{-6}$. Here we assume that the matrix $A$ given as input verifies the assumption of this problem.
	\end{enumerate}
\end{problem}

\vspace{1mm}


\begin{problem}[$\star$]
	Let $A \in \R^{n \times n}$ be a symmetric matrix. Define the function
	$$
	\begin{array}{cccc}
		f:& \R^n \setminus \{0\} & \to & \R \\
		  & x & \mapsto & \displaystyle \frac{x^{\sT} A x}{x^{\sT} x}.
	\end{array}
	$$
	Show that $f$ has a maximum at some $x_{\star} \in \R^n \setminus \{0\}$ and that $x_{\star}$ verifies
	$$
	A x_{\star} = \lambda x_{\star}, \qquad \text{where} \quad \lambda = f(x_{\star}).
	$$
\end{problem}
\vspace{1cm}
\centerline{\pgfornament[width=7cm]{87}}

%\bibliographystyle{plain}
%\bibliography{./references.bib}
\end{document}
