\documentclass[11pt,nocut]{article}

\usepackage{../latex_style/packages}
\usepackage{../latex_style/notations}

\title{\vspace{-2.0cm}%
	Optimization and Computational Linear Algebra for Data Science\\
	Lecture 2: Linear transformations}
	\author{LÃ©o \textsc{Miolane} \ $\cdot$ \ \texttt{leo.miolane@gmail.com}}
\date{\today}

\begin{document}
\maketitle
\input{../preamble.tex}
\section{Linear transformations}

\begin{definition}[Linear transformation]
	A function $L: \R^m \to \R^n$ is linear if
	\begin{enumerate}[label=(\roman*)]
		\item for all $v \in \R^m$ and all $\alpha \in \R$ we have $L(\alpha v) = \alpha L(v)$ and
		\item for all $v,w \in \R^m$ we have $L(v + w) = L(v) + L(w)$.
	\end{enumerate}
\end{definition}

Notice that $L: \R^m \to \R^n$ is linear if and only if $L(\alpha v + w) = \alpha L(v) + L(w)$ for all $v,w \in \R^m$ and all $\alpha \in \R$.

\begin{proposition}
	The set $\mathcal{L}(\R^m, \R^n)$ of all linear transformations from $\R^m$ to $\R^n$ is a vector space.
\end{proposition}

\begin{proposition}
	If $L: \R^m \to \R^n$ and $M: \R^n \to \R^k$ are two linear transformations, then the composite function $M \circ L: \R^m \to \R^k$ is also linear.
\end{proposition}

\begin{theorem}[Equality on a basis implies equality everywhere]\label{th:basis_cara}
	Let $L$ and $M$ be two linear transformations from $\R^m$ to $\R^m$.
	Let $(v_1, \dots, v_m)$ be a basis of $\R^m$ and supose that for all $i \in \{1, \dots,m\}$ we have
	$$
	L(v_i) = M(v_i).
	$$
	Then $L = M$, i.e.\ $L(v) = M(v)$ for all $v \in \R^m$.
\end{theorem}

\section{Matrix representation}

From Theorem \ref{th:basis_cara} we that a linear transformation $L: \R^m \to \R^n$ is uniquely characterized by the image $L(v_1), \dots, L(v_m)$ of any basis $(v_1, \dots, v_m)$ of the input space.

We consider the canonical basis $(e_1, \dots, e_m)$ of $\R^m$ and encode $L$ by a $n \times m$ matrix (that we will write also $L$) whose colums are $L(e_1), \dots, L(e_m)$:
$$
L =
\begin{pmatrix}
	| & | & & | \\
	L(e_1) & L(e_2) & \cdots& L(e_m) \\
	| & | & & |
\end{pmatrix}
= 
\begin{pmatrix}
	L_{1,1} & L_{1,2} & \cdots & L_{1,m} \\
	L_{2,1} & L_{2,2} & \cdots & L_{2,m} \\
	\vdots & \vdots & \ddots & \vdots \\
	L_{n,1} & L_{n,2} & \cdots & L_{n,m} \\
\end{pmatrix}
$$
where we write $L(e_j) = 
\begin{pmatrix}
	L_{1,j} \\
	L_{2,j}\\
	\vdots \\
	L_{n,j}
\end{pmatrix}$.
The matrix $L$ is called the (canonical) matrix of the linear transformation $L$.

\begin{definition}[Matrix product]
	Let $L \in \R^{n \times m}$ and $M \in \R^{k \times n}$. The product $M L$ is the $k \times m$ matrix defined by
	$$
	(ML)_{i,j} = \sum_{r=1}^n M_{i,r} L_{r,j} \quad \text{for all} \quad 1 \leq i \leq k, \quad 1 \leq j \leq m.
	$$
\end{definition}

\begin{proposition}[Matrix product means composition of linear transformations]
\end{proposition}


	\vspace{1cm}
	\centerline{\pgfornament[width=7cm]{71}}

%\bibliographystyle{plain}
%\bibliography{./references.bib}
\end{document}
