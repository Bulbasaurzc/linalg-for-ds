\documentclass{beamer}

\usepackage{../../../latex_style/beamerthemeExecushares}
\usepackage{../../../latex_style/notations}

\title{Session 4: Norms and inner-products}
\subtitle{Optimization and Computational Linear Algebra for Data Science}
\author{LÃ©o Miolane}
\date{}

\setcounter{showSlideNumbers}{1}

\begin{document}
\setcounter{showProgressBar}{0}
\setcounter{showSlideNumbers}{0}

\frame{\titlepage}

\begin{frame}
	\frametitle{Contents}
	\begin{enumerate}
		\item Norms \& inner-products
		\item Orthogonality
		\item Orthogonal projection
		\item Proof of the Cauchy-Schwarz inequality
	\end{enumerate}
\end{frame}


\setcounter{framenumber}{0}
\setcounter{showSlideNumbers}{1}
\section{Norms and inner-products}
\begin{frame}[t]{Questions}
	\grid 

	\pause
	\pause
\end{frame}

\section{Orthogonality}

\begin{frame}[t]{Definition}
	\grid 

	\vspace{-0.4cm}
	\begin{definition}
		\begin{itemize}
			\item We say that vectors $x$ and $y$ are \emph{orthogonal} if $\langle x,y \rangle = 0$. We write then $x \perp y$.
			\item We say that a vector $x$ is orthogonal to a set of vectors $A$ if $x$ is orthogonal to all the vectors in $A$. We write then $x \perp A$.
			%\item More generality we say that $A \subset V$ and $B \subset V$ are orthogonal if $\langle x,y \rangle = 0$ for all $x \in A$ and all $y \in B$. As before, we write $A \perp B$.
		\end{itemize}
	\end{definition}


	\textbf{Exercise:}
		If $x$ is orthogonal to $v_1, \dots, v_k$ then $x$ is orthogonal to any linear combination of these vectors i.e.\ $x \perp \Span(v_1, \dots, v_k)$.
\end{frame}

\begin{frame}{Pythagorean Theorem}
	\grid

	\vspace{-0.4cm}
	\begin{theorem}[Pythagorean theorem]
		Let $\| \cdot \|$ be the norm induced by $\langle \cdot, \cdot \rangle$.
		For all $x,y \in V$ we have
		$$
		x \perp y \ \Longleftrightarrow \|x+y\|^2 = \|x\|^2 + \|y\|^2.
		$$
	\end{theorem}
	\begin{proof}
		\vfill
		\vspace{4cm}
	\end{proof}
\end{frame}

\begin{frame}[t]{Application to random variables}
	\grid

\end{frame}

\begin{frame}[t]{Orthogonal \& orthonormal families}
	\grid

	\vspace{-0.4cm}
	\begin{definition}
		We say that a family of vectors $(v_1, \dots, v_k)$ is:
		\begin{itemize}
			\item \emph{orthogonal} if the vectors $v_1, \dots, v_n$ are pairwise orthogonal, i.e.\ $\langle v_i, v_j \rangle = 0$ for all $i \neq j$.
			\item \emph{orthonormal} if it is orthogonal and if all the $v_i$ have unit norm: $\|v_1\| = \dots = \|v_k\| = 1$.
		\end{itemize}
	\end{definition}
\end{frame}

\begin{frame}[t]{Coordinates in an orthonormal basis}
	\grid

	\vspace{-0.4cm}
	\begin{block}{Proposition}
		A vector space of finite dimension admits an orthonormal basis.
	\end{block}
	\begin{block}{Proposition}
		Assume that $\dim(V)=n$ and let $(v_1, \dots, v_n)$ be an \textbf{orthonormal} basis of $V$. Then the coordinates of a vector $x \in V$ in the basis $(v_1, \dots, v_n)$ are $(\langle v_1, x\rangle, \dots, \langle v_n,x \rangle)$:
		$$
		x = \langle v_1, x \rangle v_1 + \cdots + \langle v_n, x \rangle v_n.
		$$
	\end{block}

\end{frame}
\begin{frame}[t]{Coordinates in an orthonormal basis}
	\grid

\end{frame}
\begin{frame}[t]{Proof}
	\grid

\end{frame}


\section{Orthogonal projection}

\begin{frame}[t]{Picture}
	\grid

	\vspace{-0.24cm}
	From now, $\langle \cdot, \cdot \rangle$ denotes the Euclidean dot product, and $\| \cdot \|$ the Euclidean norm.
\end{frame}

\begin{frame}[t]{Orthogonal projection and distance to a subspace}
	\begin{definition}
		Let $S$ be a subspace of $\R^n$. The \emph{orthogonal projection} of a vector $x$ onto $S$ is defined as the vector $P_S(x)$ in $S$ that minimizes the distance to $x$:
		$$
		P_S(x) \defeq \argmin_{y \in S} \|x - y\|.
		$$
		The distance of $x$ to the subspace $S$ is then defined as
		$$
		d(x,S) \defeq \min_{y \in S} \| x - y \| = \|x - P_S(x)\|.
		$$
	\end{definition}
\end{frame}

\begin{frame}[t]{Computing orthogonal projections}
	\grid 

	\vspace{-0.4cm}
	\begin{block}{Proposition}
		Let $S$ be a subspace of $\R^n$ and let $(v_1, \dots, v_k)$ be an \textbf{orthonormal basis} of $S$. Then for all $x \in \R^n$,
		$$P_S(x) = \langle v_1, x \rangle v_1 + \cdots + \langle v_k, x \rangle v_k.$$
	\end{block}
\end{frame}

\begin{frame}[t]{Proof}
	\grid

\end{frame}
\begin{frame}[t]{Consequence}
	\grid

\end{frame}


\begin{frame}[t]{Consequence}
	\grid

	\vspace{-0.4cm}
	\begin{block}{Corollary}
		For all $x \in \R^n$,
		\begin{itemize}
			\item $x - P_S(x)$ is orthogonal to $S$.
			\item $\|P_S(x) \| \leq \|x\|$.
		\end{itemize}
	\end{block}
\end{frame}



\section{Proof of Cauchy-Schwarz inequality}
\begin{frame}[t]{Cauchy-Schwarz inequality}
	\grid

	\vspace{-0.4cm}
\begin{theorem}
	Let $\| \cdot \|$ be the norm induced by the inner product $\langle \cdot , \cdot \rangle$ on the vector space $V$. Then for all $x,y \in V$:
	\begin{equation}\label{eq:cauchy_schwarz}
	| \langle x,y \rangle | \leq \|x\| \, \|y\|.
	\end{equation}
	Moreover, there is equality in \eqref{eq:cauchy_schwarz} if and only if $x$ and $y$ are linearly dependent, i.e.\ $x = \alpha y$ or $y = \alpha x$ for some $\alpha \in \R$.
\end{theorem}
\end{frame}
\begin{frame}[t]
	\frametitle{Proof}
	\grid

	\pause
\end{frame}

\appendix
\backupbegin
\begin{frame}[t]
	\frametitle{Questions?}
	\grid

	\pause
\end{frame}
\begin{frame}[t]{Orthogonal matrices}
	\grid

	\begin{definition}
		A matrix $A \in \R^{n \times n}$ is called an \emph{orthogonal matrix} if its columns are an orthonormal family.
	\end{definition}
\end{frame}

\begin{frame}[t]{A proposition}
	\grid

	\vspace{-0.4cm}
	\begin{block}{Proposition}
		Let $A \in \R^{n \times n}$. The following points are equivalent:
		\begin{enumerate}
			\item $A$ is orthogonal.
			\item $A^{\sT} A = \Id_n$.
			\item $A A^{\sT} = \Id_n$
		\end{enumerate}
	\end{block}
\end{frame}

\begin{frame}[t]{Orthogonal matrices \& norm}
	\grid

	\vspace{-0.4cm}
	\begin{block}{Proposition}
	Let $A \in \R^{n \times n}$ be an orthogonal matrix. Then $A$ preserves the dot product in the sense that for all $x,y \in \R^n$,
	$$
	\langle Ax, Ay \rangle = \langle x,y\rangle.
	$$
	In particular if we take $x=y$ we see that $A$ preserves the Euclidean norm: $\|Ax\| = \|x\|$.
\end{block}
\end{frame}
\backupend

\end{document}
