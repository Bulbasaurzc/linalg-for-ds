\documentclass{beamer}

\usepackage{../../../latex_style/beamerthemeExecushares}
\usepackage{../../../latex_style/notations}

\title{Session 2: Linear transformations and matrices}
\subtitle{Optimization and Computational Linear Algebra for Data Science}
\author{LÃ©o Miolane}
\date{}

\setcounter{showSlideNumbers}{1}

\begin{document}
\setcounter{showProgressBar}{0}
\setcounter{showSlideNumbers}{0}

\frame{\titlepage}

\begin{frame}
	\frametitle{Contents}
	\begin{enumerate}
		\item Recap of the videos
		\item Operation on matrices
		\item Kernel and Image
		\item Why do we care about all these things ? \below{Solving linear systems}
	\end{enumerate}
\end{frame}


\setcounter{framenumber}{0}
\setcounter{showSlideNumbers}{1}

\section{Linear maps \& matrices}

\begin{frame}[t]{Two sides of the same coin}
	\grid
	\begin{columns}
		\hspace{-0.85cm}
		\begin{column}{0.5\textwidth}
			\vspace{-0.5cm}
			\begin{center}
				\bf Linear map
			\end{center}
			$$
			L: \R^m \to \R^n
			$$
			\vspace{6cm}
		\end{column}
		\vrule
		\begin{column}{0.5\textwidth}
			\vspace{-0.5cm}
			\begin{center}
				\bf Matrix
			\end{center}
			$$
			L \in \R^{n \times m}
			$$
			\vspace{6cm}
		\end{column}
	\end{columns}
	\pause
\end{frame}

\begin{frame}[t]{Rotations in $\R^2$}
	\grid

	Let $\theta \in \R$.
	The rotation $R_{\theta}: \R^2 \to \R^2$ of angle $\theta$ about the origin is linear.
	\\
	\vspace{0.3cm}
	\textbf{Exercise}: what is the canonical matrix of $R_{\theta}$ ?
\end{frame}

\section{Operations on matrices}

\begin{frame}[t]{Addition and scalar multiplication}
	\grid

	\vspace{0.3cm}
	\begin{itemize}
		\item
			Sum of two matrices of the \textbf{same} dimensions:
			{\small
				$$
				\!\!\!\!\!\!\!\!\!\!\!\!
				\hspace*{-0.7cm}
				\begin{pmatrix}
					a_{1,1}  & \cdots & a_{1,m} \\
					\vdots & \ddots & \vdots \\
					a_{n,1} & \cdots & a_{n,m} \\
				\end{pmatrix}
				+
				\begin{pmatrix}
					b_{1,1}  & \cdots & b_{1,m} \\
					\vdots & \ddots & \vdots \\
					b_{n,1} & \cdots & b_{n,m} \\
				\end{pmatrix}
				=
				\begin{pmatrix}
					a_{1,1} + b_{1,1}  & \cdots & a_{1,m} + b_{1,m} \\
					\vdots & \ddots & \vdots \\
					a_{n,1} + b_{n,1} & \cdots & a_{n,m} + b_{n,m} \\
				\end{pmatrix}
				$$
			}
			\vspace{0.6cm}

		\item
			{
				Multiplication by a scalar $\lambda$:
				$$
				\lambda
				\begin{pmatrix}
					a_{1,1}  & \cdots & a_{1,m} \\
					\vdots & \ddots & \vdots \\
					a_{n,1} & \cdots & a_{n,m} \\
				\end{pmatrix}
				=
				\begin{pmatrix}
					\lambda a_{1,1}  & \cdots & \lambda a_{1,m} \\
					\vdots & \ddots & \vdots \\
					\lambda a_{n,1} & \cdots & \lambda a_{n,m} \\
				\end{pmatrix}
				$$
			}
	\end{itemize}
\end{frame}
\begin{frame}[t]{A new vector space!}
	\vspace{-0.3cm}
	\begin{block}{Proposition}
		\begin{itemize}
			\item $\R^{n \times m}$ is a vector space. 
			\item $\dim(\R^{n \times m}) = $
		\end{itemize}
	\end{block}
	\begin{proof}
		\vspace{4cm}
		\vfill
	\end{proof}
\end{frame}
\begin{frame}[t]{Product of two matrices}
	\grid

	\textbf{Warning:}
	{\small
		$$
		\!\!\!\!\!\!\!\!\!\!\!\!
		\begin{pmatrix}
			a_{1,1}  & \cdots & a_{1,m} \\
			\vdots & \ddots & \vdots \\
			a_{n,1} & \cdots & a_{n,m} \\
		\end{pmatrix}
		\times
		\begin{pmatrix}
			b_{1,1}  & \cdots & b_{1,m} \\
			\vdots & \ddots & \vdots \\
			b_{n,1} & \cdots & b_{n,m} \\
		\end{pmatrix}
		\neq
		\begin{pmatrix}
			a_{1,1} \times b_{1,1}  & \cdots & a_{1,m} \times b_{1,m} \\
			\vdots & \ddots & \vdots \\
			a_{n,1} \times b_{n,1} & \cdots & a_{n,m} \times b_{n,m} \\
		\end{pmatrix}
		$$
	}
\end{frame}

\begin{frame}[t]{Matrix product}
	\grid

	Let $L \in \R^{n \times m}$ and $M \in \R^{m \times k}$. 
	%\vspace{2cm}
	\begin{definition}[Matrix product]
		The matrix product $LM$ is the $n \times k$ matrix of the linear map $L \circ M$.
	\end{definition}
	\vspace{2.7cm}
\end{frame}

\begin{frame}[t]{Matrix product}
	\grid
	\vspace{-0.3cm}
	\begin{theorem}
		The entries matrix product $LM$ are given by
		\vspace{-0.2cm}
		$$
		(LM)_{i,j} 
		=\sum_{\ell = 1}^m L_{i,\ell} M_{\ell,j}, \qquad \text{for} \ \  1 \leq i \leq n \ \ \text{and} \ \ 1 \leq j \leq k.
		\vspace{-0.2cm}
		$$
	\end{theorem}
\end{frame}
\begin{frame}[t]{Rotations in $\R^2$}
	\grid

	The $R_{a}$ and $R_{b}$ denote respectively the matrix of the rotation of angle $a$ and $b$ about the origin, in $\R^2$.
	\\
	\vspace{0.3cm}
	\textbf{Exercise}:
	Compute the product $R_a R_b$.

\end{frame}
\begin{frame}[t]{Matrix product properties}
	\grid

\end{frame}

\section{Kernel and image}

\begin{frame}[t]{Definitions}
	Let \quad $L: \R^m \to \R^n$ \quad be a linear transformation.
	\begin{definition}[Kernel]
		The kernel $\Ker(L)$ (or nullspace) of $L$ is defined as the set of all vectors $v \in \R^m$ such that $L(v) = 0$, i.e.
		$$
		\Ker(L) \defeq \big\{ v \in \R^m \, \big| \, L(v) = 0 \big\}.
		$$
	\end{definition}

	\begin{definition}[Image]
		The image $\Im(L)$ (or column space) of $L$ is defined as the set of all vectors $u \in \R^n$ such that there exists $v \in \R^m$ such that $L(v) = u$. 
	\end{definition}
\end{frame}



\begin{frame}[t]{Picture}
	\grid
\end{frame}

\begin{frame}[t]{Remarks}
	\grid

	Let \quad $L: \R^m \to \R^n$ \quad be a linear transformation.
	\begin{block}{Proposition}
		\begin{itemize}
			\item $\Ker(L)$ is a subspace of $\R^m$.
			\item $\Im(L)$ is a subspace of $\R^n$.
		\end{itemize}
	\end{block}
	\textbf{Remark:} $\Im(L)$ is also the Span of the columns of the matrix representation of $L$.

\end{frame}

\begin{frame}[t]{Example: orthogonal projection}
	\grid

	Consider \quad $L: \R^2 \to \R^2$ \quad to be the orthogonal projection onto the $x$-axis.
\end{frame}


\section{Why do we care about this ?}

\begin{frame}[t]{Linear systems}
	\grid

	Assume that we given a dataset:
	$$
	a_i = (a_{i,1}, \dots, a_{i,m}) \in \R^m, \quad y_i \in \R \qquad \text{for} \quad i = 1, \dots, n.
	$$
	We would like to find $x \in \R^m$ such that
	$$
	x_1 a_{i,1} + \cdots + x_m a_{i,m} = y_i \qquad \text{for all} \quad i \in \{1, \dots, n\}.
	$$
\end{frame}

\begin{frame}[t]{Matrix notation}
	\grid

	Let us write
	$$
	A = 
	\begin{pmatrix}
		a_{1,1}  & \cdots & a_{1,m} \\
		\vdots & \ddots & \vdots \\
		a_{n,1} & \cdots & a_{n,m} \\
	\end{pmatrix} \in \R^{n \times m}
	\qquad \text{and} \qquad
	y = 
	\begin{pmatrix}
		y_1 \\
		\vdots \\
		y_n
	\end{pmatrix}
	\in \R^n.
	$$
\end{frame}

\begin{frame}[t]{3 possible cases}
	\grid

\end{frame}


\begin{frame}[t]{Gaussian elimination}
	\grid
	$$
	A = 
	\begin{pmatrix}
		1  & -1 & 0 & 1 \\
		2  & 0 & 1 & -1 \\
		-1  & 5 & 2 & 0 
	\end{pmatrix} \in \R^{n \times m}
	\qquad \text{and} \qquad
	y = 
	\begin{pmatrix}
		1 \\
		1 \\
		-1
	\end{pmatrix}
	\in \R^n.
	$$

\end{frame}
\begin{frame}[t]{Gaussian elimination}
	\grid
	\pause
	\pause
\end{frame}
\appendix
\backupbegin
\begin{frame}
	\frametitle{Questions?}
\end{frame}
\backupend

\end{document}
