\documentclass{beamer}

\usepackage{../../../latex_style/beamerthemeExecushares}
\usepackage{../../../latex_style/notations}

\title{Session 3: The rank}
\subtitle{Optimization and Computational Linear Algebra for Data Science}
\author{LÃ©o Miolane}
\date{}

\setcounter{showSlideNumbers}{1}

\begin{document}
\setcounter{showProgressBar}{0}
\setcounter{showSlideNumbers}{0}

\frame{\titlepage}

\begin{frame}
	\frametitle{Contents}
	\begin{enumerate}
		\item The rank
		\item The rank-nullity Theorem
		\item More on the inverse of a matrix
		\item Transpose of a matrix
		\item Why do we care about all these things ? \below{Is the rank useful in practice?}
	\end{enumerate}
\end{frame}


\setcounter{framenumber}{0}
\setcounter{showSlideNumbers}{1}

\section{The rank}

\begin{frame}[t]{Recap of the videos}
	\vspace{-0.4cm}
	\begin{definition}
		We define the rank of a family $x_1, \dots, x_k$ of vectors of $\R^n$ as the dimension of its span:
		\vspace{-0.3cm}
		$$
		\rank(x_1, \dots, x_k) \defeq \dim (\Span(x_1, \dots, x_k)).
		\vspace{-0.3cm}
		$$
	\end{definition}

	\begin{definition}
		Let $M \in \R^{n \times m}$. Let $c_1, \dots, c_m \in \R^n$ be its columns.
		We define
		\vspace{-0.3cm}
		$$
		\rank(M) \defeq \rank(c_1, \dots, c_m) = \dim(\Im(M)).
		\vspace{-0.3cm}
		$$
	\end{definition}

	\begin{block}{Proposition}
		Let $M \in \R^{n \times m}$. Let $r_1, \dots, r_n \in \R^m$ be the rows of $M$ and $c_1, \dots, c_m \in \R^n$ be its columns.
		Then we have
		\vspace{-0.3cm}
		$$
		\rank(r_1, \dots, r_n) = \rank(c_1, \dots, c_m) = \rank(M).
		\vspace{-0.3cm}
		$$
	\end{block}

\end{frame}

\begin{frame}[t]{How do we compute the rank ?}
	\grid

	For $v_1, \dots, v_k \in \R^n$, and $\alpha \in \R \setminus \{0\}, \, \beta \in \R$ we have
	\begin{align*}
		\rank(v_1, \dots, v_k)
		&=
		\rank(v_1, \dots, v_{i-1}, \, \alpha v_i \, , v_{i+1}, \dots, v_k)\\
		&=
		\rank(v_1, \dots, v_{i-1}, \, v_i + \beta v_j \, , v_{i+1}, \dots, v_k)
	\end{align*}
	\vspace{2cm}
	\\
	As a consequence, the Gaussian elimination method keeps the rank of a matrix unchanged!
\end{frame}

\begin{frame}[t]{Example}
	\grid

	Let's compute the rank of \quad
	$\displaystyle
	A = 
	\begin{pmatrix}
		1  & -1 & 0 & 1 \\
		2  & 0 & 1 & -1 \\
		-1  & 5 & 2 & 0 
	\end{pmatrix}
	$
\end{frame}

\begin{frame}[t]{Example}
	\grid

\end{frame}

\section{The rank-nullity Theorem}
\begin{frame}[t]{Rank-nullity Theorem}
	\grid

	\vspace{-0.3cm}
	\begin{theorem}
		Let $L: \R^m \to \R^n$ be a linear transformation. Then
		$$
		\rank(L) + \dim(\Ker(L)) = m.
		$$
	\end{theorem}

\end{frame}

\begin{frame}[t]{Proof sketch on an example}
	\grid

	Let us solve the linear system $Ax = 0$.
	\begin{columns}
		\hspace*{-0.7cm}
		\begin{column}{0.40\textwidth}
			$$
			\left(
				\begin{array}{cccc}
					1  & -1 & 0 & 1 \\
					2  & 0 & 1 & -1 \\
					-1  & 5 & 2 & 0 
				\end{array}
				\middle|
				\begin{array}{c}
					0 \\
					0 \\
					0
				\end{array}
			\right)
			$$
			$$
			\left(
				\begin{array}{cccc}
					1  & -1 & 0 & 1 \\
					0  & 2 & 1 & -3 \\
					0  & 0 & 0 & 7 
				\end{array}
				\middle|
				\begin{array}{c}
					0 \\
					0 \\
					0
				\end{array}
			\right)
			{\color{red}
				\begin{array}{l}
					(R_1)\\
					(R_2)\\
					(R_3) -2 (R_2)
				\end{array}
			}
			$$
		\end{column}
		\begin{column}{0.55\textwidth}
			\vspace{-1.9cm}
			$$
			\left(
				\begin{array}{cccc}
					1  & -1 & 0 & 1 \\
					0  & 2 & 1 & -3 \\
					0  & 4 & 2 & 1 
				\end{array}
				\middle|
				\begin{array}{c}
					0 \\
					0 \\
					0
				\end{array}
			\right)
			{\color{red}
				\begin{array}{l}
					(R_1)\\
					(R_2) - 2 (R_1) \\
					(R_3) + (R_1)
				\end{array}
			}
			$$
		\end{column}
	\end{columns}


\end{frame}

\begin{frame}[t]{Proof of the rank-nullity Theorem}
	\grid

	\pause
	\pause
	\pause
\end{frame}

\section{Invertible matrices}

\begin{frame}[t]{Invertible matrices}
	\grid

	\vspace{-0.3cm}

	\begin{definition}[Matrix inverse]\label{prop:matrix_inverse}
		A \textbf{square} matrix $M \in \R^{n \times n}$ is called \emph{invertible} if there exists a matrix $M^{-1} \in \R^{n \times n}$ such that 
		$$
		M M^{-1} = M^{-1} M = \Id_n.
		$$
		Such matrix $M^{-1}$ is unique and is called the \emph{inverse} of $M$.
	\end{definition}
	\textbf{Exercise}: Let $A,B \in \R^{n \times n}$. Show that if $AB = \Id_n$ then $BA = \Id_n$.

\end{frame}

\begin{frame}[t]{Invertible matrices}
	\grid

	\vspace{-0.3cm}

	\begin{block}{Theorem}
		Let $M \in \R^{n \times n}$. The following points are equivalent:
		\begin{enumerate}
			\item \label{item:th_i} $M$ is invertible.
			\item For all $y \in \R^n$, there exists a unique $x \in \R^n$ such that $Mx=y$.
			\item \label{item:th_ii} $\rank(M) = n$.
			\item \label{item:th_iii} $\Ker(M) = \{0\}$.
		\end{enumerate}
	\end{block}

\end{frame}
\begin{frame}[t]{Proof}
	\grid

	\pause
	\pause
	\pause
\end{frame}


\section{Transpose of a matrix}

\begin{frame}[t]{Transpose of a matrix}
	\grid

	\vspace{-0.4cm}
	\begin{definition}
		Let $M \in \R^{n \times m}$. We define its \emph{transpose} $M^{\sT} \in \R^{m \times n}$ by
		$$
		(M^{\sT})_{i,j} = M_{j,i}
		$$
		for all $i \in \{1, \dots, m\}$ and $j \in \{1, \dots, n\}$.
	\end{definition}

	\vspace{3cm}
	\textbf{Remark:}
	\begin{itemize}
		\item We have $(M^{\sT})^{\sT} = M$. 
		\item The mapping $M \mapsto M^{\sT}$ is linear.
	\end{itemize}
\end{frame}


\begin{frame}[t]{Properties of the transpose}
	\grid

	\vspace{-0.4cm}
	\begin{block}{Proposition}
		For all $A \in \R^{n \times m}$, \quad
		$\rank(A) = \rank(A^{\sT})$.
	\end{block}

	\begin{block}{Proposition}
		Let $A \in \R^{n \times m}$ and $B \in \R^{m \times k}$. Then
		\vspace{-0.3cm}
		$$
		(AB)^{\sT} = B^{\sT} A^{\sT}.
		\vspace{-0.3cm}
		$$
	\end{block}
	\begin{proof}
		\vspace{2.5cm}
		\vfill
	\end{proof}
\end{frame}

\begin{frame}[t]{Symmetric matrices}
	\grid

	\vspace{-0.4cm}
	\begin{block}{Definition}
	A square matrix $A \in \R^{n \times n}$ is said to be \emph{symmetric} if
	$$
	\forall i,j \in \{1, \dots, n\}, \ A_{i,j} = A_{j,i}
	$$
	or, equivalently if $A = A^{\sT}$.
	\end{block}

	\textbf{Remark}: For all $M \in \R^{n \times m}$ the matrix $M M^{\sT}$ is symmetric.
\end{frame}

\section{Is the rank useful in practice?}

\begin{frame}[t]{Back to the movies ratings example}
	\grid

	\vspace{-0.3cm}
	Assume that you are given the matrix of movies ratings:
	$$
	\begin{pmatrix}
		1 & 1 & 5 & 5 & 5 \\
		2 & 2 & 2 & 0 & 0 \\
		1 & 1.001 & 5 & 5 & 5 \\
		2 & 2 & 2 & 0.0001 & 0 \\
		2.0001 & 2 & 2 & 0 & 0 
	\end{pmatrix}
	$$

	\vspace{0.5cm}
	\textbf{Goal:} how many different << user profiles >> do we have ?
\end{frame}



\begin{frame}[t]{Conclusions}
	\grid

	\begin{itemize}
		\item The rank is not <<robust>> !
			\vspace{0.3cm}
		\item We need to have a way to check if a matrix is <<approximately low-rank>>.
			\vspace{0.3cm}
		\item Equivalentely, given $m$ vectors, one would like to be able to see if there exists a subspace of dimension $k \ll m$ from which the vectors are << not far >>.
			\vspace{0.3cm}
		\item The singular value decomposition (lecture 6-7) will solves our problems !
	\end{itemize}
\end{frame}


\appendix
\backupbegin
\begin{frame}[t]
	\frametitle{Questions?}
	\grid

	\pause
	\pause
\end{frame}
\backupend

\end{document}
