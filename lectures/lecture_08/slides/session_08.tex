\documentclass{beamer}

\usepackage{../../../latex_style/beamerthemeExecushares}
\usepackage{../../../latex_style/notations}
%\usepackage{../../../latex_style/packages_list}

\title{Session 8: SVD, spectral clustering on graphs}
\subtitle{Optimization and Computational Linear Algebra for Data Science}
\author{LÃ©o Miolane}
\date{}


\setcounter{showSlideNumbers}{1}

\begin{document}
\setcounter{showProgressBar}{0}
\setcounter{showSlideNumbers}{0}

\frame{\titlepage}

\begin{frame}
	\frametitle{Contents}
	\begin{enumerate}
		\item Singular Value Decomposition
		\item Graphs and Graph Laplacian
		\item Spectral clustering
	\end{enumerate}
\end{frame}

\begin{frame}{Midterm next week}
	\begin{itemize}
		\item Thu.\ Oct.\ 29, the questions have to be downloaded from Gradescope between 00:01 AM and 9:59 PM.
			\vspace{2mm}
		\item \textbf{Duration:} 1 hour and 40 minutes to work on the problems + 20 minutes to scan and upload your work.
			\vspace{2mm}
		\item Upload your work \textbf{as a single PDF}.
			\vspace{2mm}
		\item In case the upload does not work for you, \textbf{email me your work}.
	\end{itemize}
\end{frame}

\setcounter{framenumber}{0}
\setcounter{showSlideNumbers}{1}


\section{Singular Value Decomposition}


\begin{frame}[t]{Singular Value decomposition}
	\grid

	\vspace{-0.3cm}
	\begin{block}{Theorem}
		Let $A \in \R^{n \times m}$. Then there exists two orthogonal matrices $U \in \R^{n \times n}$ and $V \in \R^{m \times m}$ and a matrix $\Sigma \in \R^{n \times m}$ such that $\Sigma_{1,1} \geq \Sigma_{2,2}  \geq \cdots \geq 0$ and $\Sigma_{i,j} = 0$ for $i\neq j$, that verify
		$$
		A = U \Sigma V^{\sT}.
		$$
	\end{block}
\end{frame}

\begin{frame}[t]{Remarks}
	\grid

\end{frame}

\begin{frame}[t]{Low-rank approximation}
	\grid

	\vspace{-0.2cm}
	\hspace{-0.5cm}How can we approximate a matrix $A$ by a matrix of <<small>> rank ?

\end{frame}

\section{Graphs and Graph Laplacian}

\begin{frame}[t]{Graphs}
	\grid

\end{frame}

\begin{frame}[t]{Graph Laplacian}
	\grid

	\vspace{-0.4cm}
	\begin{definition}
		The Laplacian matrix of $G$ is defined as
		$$
		L = D - A.
		$$
	\end{definition}

	\pause
	\vspace{-0.5cm}
	\begin{exampleblock}{}
		For all $x \in \R^n$,
		$\displaystyle \qquad
		x^{\sT} L x = \sum_{i \sim j} (x_i - x_j)^2.
		\vspace{-0.1cm}
		$
	\end{exampleblock}
\end{frame}


\begin{frame}[t]{Properties of the Laplacian}
	\grid

	\vspace{-0.8cm}
	\begin{exampleblock}{}
		For all $x \in \R^n$,
		$\displaystyle \qquad
		x^{\sT} L x = \sum_{i \sim j} (x_i - x_j)^2.
		%\vspace{-0.3cm}
		$
	\end{exampleblock}


	\pause
\end{frame}

\begin{frame}[t]{Algebraic connectivity}
	\grid

	\begin{block}{Proposition}
\begin{itemize}
	\item The multiplicity of the eigenvalue $0$ of $L$ (i.e.\ the number of $i$ such that $\lambda_i = 0$) is equal to the number of connected components of $G$.
	\item In particular, $G$ is connected if and only if \ $\lambda_2 > 0$.
\end{itemize}
\end{block}

\vspace{0.5cm}
\begin{itemize}
	\item $\lambda_2$ is sometimes called the <<algebraic connectivity>> of $G$ and measures somehow how well $G$ is connected.
	\item From now, we assume that $G$ is connected, i.e.\ $\lambda_2 >0$.
\end{itemize}

\vspace{0.5cm}
\textbf{Exercise}: show that $\lambda_2$ increases when one adds edges to $G$.
\end{frame}

\section{Spectral clustering with the Laplacian}

\begin{frame}[t]{Spectral \ clustering \ algorithm}
	\grid

	\vspace{-0.2cm}
	\textbf{Input:} Graph Laplacian $L$, number of clusters $k$
	\begin{enumerate}
		\item Compute the first $k$ orthonormal eigenvectors $v_1, \dots, v_k$ of the Laplacian matrix $L$.
			\vspace{0.1cm}
		\item Associate to each node $i$ the vector $\ x_i = \big(v_{2}(i), \dots, v_k(i) \big)$.
			\vspace{0.1cm}
		\item Cluster the points $x_1, \dots, x_n$ with (for instance) the $k$-means algorithm.
			\vspace{0.1cm}
		\item Deduce a clustering of the nodes of the graph.
	\end{enumerate}

\end{frame}

\begin{frame}[t]{The case of two groups}
	\grid

	\textbf{For $k=2$ groups:}
	\begin{enumerate}
		\item Compute the second eigenvector $v_2$ of the Laplacian matrix $L$.
			\vspace{0.1cm}
		\item Associate to each node $i$ the number $\ x_i = v_{2}(i)$.
			\vspace{0.1cm}
		\item Cluster the nodes in:
			$$
			S = \{ i \, | \, v_2(i) \geq \delta \} \qquad \text{and} \qquad
			S^c = \{ i \, | \, v_2(i) < \delta \},
			$$
			for some $\delta \in \R$.
	\end{enumerate}

\end{frame}

\begin{frame}[t]{Cut of a partition}
	\grid

\end{frame}
\begin{frame}[t]{Minimal cut problem}
	\grid

\end{frame}

\begin{frame}[t]{<< Min-Cut >> is NP-Hard}
	\grid
	\vspace{-0.8cm}
	\begin{exampleblock}{}
		\textbf{Goal:}
		$\displaystyle \qquad \text{minimize}
		\qquad
		x^{\sT} L x 
		\qquad
		\text{subject to}
		\ \
		\begin{cases}
			x \in \{-1,1\}^n \\
			x \perp (1, \dots, 1).
		\end{cases}
		$
	\end{exampleblock}

\end{frame}

\begin{frame}[t]{Spectral clustering as a <<relaxation>>}
	\grid

	\vspace{-0.8cm}
	\begin{exampleblock}{}
		\textbf{Idea:} \ We first solve the << relaxed >> problem:
		\vspace{-0.2cm}
		$$
		\text{minimize}
		\qquad
		v^{\sT} L v 
		\qquad
		\text{subject to}
		\ \
		\begin{cases}
			\|v\| = \sqrt{n} \\
			v \perp (1, \dots, 1).
		\end{cases}
		\vspace{-0.4cm}
		$$
	\end{exampleblock}

\end{frame}

\appendix
\backupbegin
\begin{frame}[t]
	\frametitle{Questions?}
	\grid

	\pause
\end{frame}
\backupend

\end{document}
