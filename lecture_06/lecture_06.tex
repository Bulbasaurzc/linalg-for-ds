\documentclass[11pt,nocut]{article}

\usepackage{../latex_style/packages}
\usepackage{../latex_style/notations}
\externaldocument{../lecture_02/lecture_02}
\externaldocument{../lecture_04/lecture_04}


\title{\vspace{-2.0cm}%
	Optimization and Computational Linear Algebra for Data Science\\
Lecture 6: Singular value decomposition}
\author{LÃ©o \textsc{Miolane} \ $\cdot$ \ \texttt{leo.miolane@gmail.com}}
\date{\today}

\begin{document}
\maketitle
\input{../preamble.tex}

\section{Eigenvalues and eigenvectors}

\begin{definition}
	Let $A \in \R^{n \times n}$. A \textbf{non-zero} vector $v \in \R^n$ is said to be an \emph{eigenvector} of $A$ is there exists $\lambda \in \R$ such that
	$$
	A v = \lambda v.
	$$
	The scalar $\lambda$ is called the eigenvalue (of $A$) associated to $v$.
\end{definition}


\begin{theorem}[Spectral Theorem]\label{th:spectral}
	Let $A \in \R^{n \times n}$ be a \textbf{symmetric} matrix. Then there is a orthonormal basis of $\R^n$ composed of eigenvectors of $A$.
\end{theorem}

Given an $n \times n$ symmetric matrix $A$, Theorem \ref{th:spectral} tells us that one can find an orthonormal basis $(v_1, \dots, v_n)$ of $\R^n$ and scalars $\lambda_1, \dots, \lambda_n \in \R$ such that for all $i \in \{1, \dots, n\}$,
$$
A v_i = \lambda_i v_i.
$$
Let $P$ be the $n \times n$ matrix whose columns are $v_1, \dots, v_n$. Since $(v_1, \dots, v_n)$ is an orthonormal basis, we get that $P$ is an orthogonal matrix. Let $D = {\rm Diag}(\lambda_1, \dots, \lambda_n)$ and compute
$$
A P
= 
A 
\begin{pmatrix}
	| & | & & | \\
	v_1 & v_2 & \cdots& v_n \\
	| & | & & |
\end{pmatrix}
= 
\begin{pmatrix}
	| & | & & | \\
	Av_1 & Av_2 & \cdots& Av_n \\
	| & | & & |
\end{pmatrix}
= 
\begin{pmatrix}
	| & | & & | \\
	\lambda_1 v_1 & \lambda_2 v_2 & \cdots& \lambda_n v_n \\
	| & | & & |
\end{pmatrix}
= P D.
$$
By multiplying by $P^{\sT}$ on both sides, we get $A P P^{\sT} = P D P^{\sT}$. Recall now that $P$ is orthogonal, therefore $P P^{\sT} = \Id_n$. We conclude that $A = P D P^{\sT}$.

\begin{theorem}[Spectral Theorem, matrix formulation]\label{th:spectral2}
	Let $A \in \R^{n \times n}$ be a symmetric matrix. Then there exists an orthogonal matrix $P$ and a diagonal matrix $D$ of sizes $n \times n$, such that
	$$
	A = P D P^{\sT}.
	$$
\end{theorem}

\begin{proposition}
	Let $A$ be a $n \times n$ symmetric matrix and let $\lambda_1 \geq \cdots \geq \lambda_n$ be its $n$ eigenvalues and $v_1, \dots, v_n$ be the associated orthonormal family of eigenvectors. Then 
	$$
	v_1 = \argmax_{\|v\| = 1} v^{\sT} A v\,,
	\qquad \text{and for} \ k=2, \dots n, \qquad
	v_{k} = \argmax_{\|v\| = 1, \, v \perp v_1, \dots, v_{k-1}} v^{\sT} A v.
	$$
\end{proposition}

\begin{remark} Applying the proposition above to the matrix $-A$ which is symmetric with eigenvalues $-\lambda_n \geq \cdots \geq -\lambda_1$ and associated eigenvectors $v_n, \dots, v_1$, we get
	$$
	v_n = \argmin_{\|v\| = 1} v^{\sT} A v\,,
	\qquad \text{and for} \ k=1, \dots , n-1 \qquad
	v_{k} = \argmin_{\|v\| = 1, \, v \perp v_{k+1}, \dots, v_{n}} v^{\sT} A v.
	$$
\end{remark}


\section{Singular value decomposition}

Let $a_1, \dots, a_n \in \R^d$ be $n$ points in $d$ dimension.

The goal of Singular Value Decomposition (SVD) is to find the $k$-dimensional subspace (for $k = 1, \dots, n$) that fits ``the best'' these $n$ data points. By ``best'', we mean here the $k$-dimensional subspace $S$ that minimize the sum of the square distances to the $n$ points:
\begin{equation}\label{eq:fit}
	\text{minimize} \ \ \sum_{i=1}^n d(a_i, S)^2 \ \ \text{with respect to} \ S \ \text{subspace of dimension} \ k.
\end{equation}
In this case we have for all $i \in \{1, \dots, n\}$,
$$
d(a_i, S)^2 = \| a_i - P_{S}(a_i) \|^2 = \|a_i\|^2 - \| P_{S}(a_i) \|^2,
$$
by Pythagorean Theorem (recall that $P_{S}(a_i) \perp (a_i - P_S(a_i))$). Since $v_1$ is of unit norm, $P_{S}(a_i) = \langle v_1, a_i \rangle v_1$, hence:
$$
d(a_i, S)^2 =  \|a_i\|^2 - \langle v_1, a_i \rangle^2.
$$
Minimizing \eqref{eq:fit} is therefore equivalent to maximize
\begin{equation}\label{eq:fit2}
\sum_{i=1}^n \| P_{S}(a_i) \|^2.
\end{equation}
Let us fix an orthonormal basis $(v_1, \dots, v_k)$ of $S$. Then for all $x \in \R^d$,
$P_S(x)= \langle v_1, x \rangle v_1 + \cdots + \langle v_k, x \rangle v_k$, hence
\begin{equation}\label{eq:fit3}
\sum_{i=1}^n \| P_{S}(a_i) \|^2 = \sum_{i=1}^n \sum_{j = 1}^k \langle a_i, v_j \rangle^2
= \| A v_1 \|^2 + \cdots + \| A v_k \|^2,
\end{equation}
where $A$ is the $n \times d$ matrix whose rows are $a_1, \dots, a_n$.
Consequently, minimizing \eqref{eq:fit} is equivalent to maximizing \eqref{eq:fit3} over all orthonormal families $(v_1, \dots, v_k)$.
\\

For $k=1$, a subspace of dimension $1$ that minimizes \eqref{eq:fit} is therefore $\Span(v_1)$ where
\begin{equation}\label{eq:def_first_singular}
v_1 \defeq \argmax_{\|v\| = 1} \|A v\|.
\end{equation}
If we now want to solve the problem for $k=2$, a natural candidate for the subspace $S$ would be $S = \Span(v_1, v_2)$ where
\begin{equation}\label{eq:def_second_singular}
v_2 \defeq \argmax_{\|v\| = 1, \, v \perp v_1} \| A v \|.
\end{equation}
We can follow this greedy strategy for $k = 3, \dots, n$ and define recursively
\begin{equation}\label{eq:def_singular}
v_{k} \defeq \argmax_{\|v\| = 1, \, v \perp v_1, \dots, v_{k-1}} \| A v \|.
\end{equation}
\begin{definition}
	\begin{itemize}
		\item The vectors $v_1, \dots, v_n$ are called \emph{singular vectors} of the matrix $A$.
		\item The non-negative numbers $\sigma_k \defeq \|A v_k\|$ are called the \emph{singular values} of $A$.
	\end{itemize}
\end{definition}
Of course \eqref{eq:def_first_singular}-\eqref{eq:def_singular} admits many other maximizers (for instance $-v_k$), so \textbf{the singular vectors are not uniquely defined}.
\\

It is not a priori obvious (except for $k=1$) that $S = \Span(v_1, \dots, v_k)$ is a minimizer of \eqref{eq:fit} over all the subspaces of dimension $k$.
We need the following lemma.

\begin{lemma}\label{lem:svd_rec}
	Let $k \in \{2, \dots, k\}$. Assume that $(v_1, \dots, v_{k-1})$ is an orthonormal family that maximizes \eqref{eq:fit3}.
	Define 
	$$
	v_k = \argmax_{\|v\| = 1, \, v \perp \Span(v_1, \dots, v_{k-1})} \|Av\|.
	$$
	Then $(v_1, \dots, v_{k})$ is an orthonormal family and $\Span(v_1, \dots v_k)$ minimizes \eqref{eq:fit}, i.e.\ $(v_1, \dots, v_{k})$ maximizes \eqref{eq:fit3}.
\end{lemma}
\begin{proof}
	Let $S$ be a subspace of dimension $k$. Let $(w_1, \dots, w_k)$ be an orthonormal basis of $S$ such that $w_{k} \perp \Span(v_1, \dots, v_{k-1})$. By definition of $v_k$,
	we have $\|A w_k \| \leq \| A v_k\|$. 
	We also assumed that $(v_1, \dots, v_k)$ maximizes \eqref{eq:fit3}, so
	$$
	\|A v_1\|^2 + \cdots + \| A v_{k-1} \|^2 \geq
	\|A w_1\|^2 + \cdots+ \| A w_{k-1} \|^2.
	$$
	We conclude that
	$$
	\|A v_1\|^2 + \cdots + \| A v_{k} \|^2 \geq
	\|A w_1\|^2 + \cdots + \| A w_{k} \|^2,
	$$
	so $(v_1, \dots, v_k)$ maximizes \eqref{eq:fit3}.
\end{proof}
\\

Using Lemma \ref{lem:svd_rec} we get by induction:
\begin{proposition}
	Let $v_1, \dots, v_n$ be singular vectors of $A$ defined by \eqref{eq:def_first_singular}-\eqref{eq:def_singular}. Then for all $k \in \{1, \dots, n\}$, the subspace $\Span(v_1, \dots, v_k)$ is a solution of \eqref{eq:fit}.
\end{proposition}




\vspace{1cm}
\centerline{\pgfornament[width=7cm]{71}}

%\bibliographystyle{plain}
%\bibliography{./references.bib}
\end{document}
