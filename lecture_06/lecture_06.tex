\documentclass[11pt,nocut]{article}

\usepackage{../latex_style/packages}
\usepackage{../latex_style/notations}
\externaldocument{../lecture_02/lecture_02}
\externaldocument{../lecture_04/lecture_04}


\title{\vspace{-2.0cm}%
	Optimization and Computational Linear Algebra for Data Science\\
Lecture 6: Eigenvalues, eigenvectors and Markov chains}
\author{LÃ©o \textsc{Miolane} \ $\cdot$ \ \texttt{leo.miolane@gmail.com}}
\date{\today}

\begin{document}
\maketitle
\input{../preamble.tex}

\section{Eigenvalues and eigenvectors}

\begin{definition}
	Let $A \in \R^{n \times n}$. A \textbf{non-zero} vector $v \in \R^n$ is said to be an \emph{eigenvector} of $A$ is there exists $\lambda \in \R$ such that
	$$
	A v = \lambda v.
	$$
	The scalar $\lambda$ is called the eigenvalue (of $A$) associated to $v$. The set
	$$
	E_{\lambda}(A) = \big\{ x \in \R^n \, \big| \, Ax = \lambda x \big\} = \Ker(A-\lambda \Id)
	$$
	is called the eigenspace of $A$ associated to $\lambda$. The dimension of $E_{\lambda}(A)$ is called the multiplicity of the eigenvalue $\lambda$.
\end{definition}

\begin{remark}
	Notice that $E_{\lambda}(A)$ is a subspace of $\R^n$: any (non-zero) linear combination of eigenvectors associated with the eigenvalue $\lambda$ is also an eigenvector of $A$ associated with $\lambda$.
\end{remark}

\begin{proposition}
	Let $A \in \R^{n \times n}$. 
	Suppose that $A$ has an eigenvalue $\lambda \in \R$ and let $x \in \R^n$ be an eigenvector associated to $\lambda$.
	The following holds:
	\begin{itemize}
		\item For all $\alpha \in \R$, $\alpha \lambda$ is an eigenvalue of the matrix $\alpha A$ and $x$ is an associated eigenvector.
		\item For all $\alpha \in \R$, $\lambda + \alpha$ is an eigenvalue of the matrix $A + \alpha \Id$ and $x$ is an associated eigenvector.
		\item For all $k \in \N$, $\lambda^k$ is an eigenvalue of the matrix $A^k$ and $x$ is an associated eigenvector.
		\item If $A$ is invertible then $1/\lambda$ is an eigenvalue of the matrix inverse $A^{-1}$ and $x$ is an associated eigenvector.
	\end{itemize}
\end{proposition}

\begin{definition}
	The set of all eigenvalues of $A$ is called the \emph{spectrum} of $A$ and denoted by $\Sp(A)$.
\end{definition}

\begin{proposition}
	A $n \times n$ matrix $A$ admits at most $n$ eigenvalues: $\# \Sp(A) \leq n$.
\end{proposition}

\section{Diagonalizable matrices}

\begin{definition}
	A matrix $A \in \R^{n \times n}$ is said to be \emph{diagonalizable} if there exists a basis $(v_1, \dots, v_n)$ of $\R^n$ consisting of eigenvectors of $A$, i.e.\ such that there exists $\lambda_1, \dots, \lambda_n \in \R$, $Av_i = \lambda_i v_i$.
\end{definition}

\begin{proposition}\label{prop:diag}
	A matrix $A \in \R^{n \times n}$ is diagonalizable if and only if there exists an invertible $n \times n$ matrix $P$ and a diagonal matrix $D = \Diag(\lambda_1, \dots, \lambda_n)$ such that
	$$
	A = P D P^{-1}.
	$$
	In this case, the $i^{\rm th}$ column of $P$ is an eigenvector of $A$ associated with the eigenvalue $\lambda_i$.
\end{proposition}

\begin{proposition}
	Let $A = P \Diag(\lambda_1,\dots,\lambda_n) P^{-1}$ (where $P \in \R^{n \times n}$ is invertible) be a diagonalizable matrix. Then
$$
\Tr(A) = \sum_{i=1}^n \lambda_i
\qquad \text{and} \qquad
\rank(A) = \# \{ i \, | \, \lambda_i \neq 0 \}.
$$
Consequently, $A$ is invertible if and only if $\lambda_i \neq 0$ for all $i$. In such case, $A^{-1} = P \Diag(\lambda_1^{-1}, \dots, \lambda_n^{-1}) P^{-1}$.
\end{proposition}

\section{Application to Markov chains}

\subsection{First definitions and properties}

A finite Markov chain is a process which moves among the elements of a finite set $E$ in the following manner: when at $x \in E$, the next position is chosen according to a fixed probability distribution $P(x, \cdot)$. More formally:

\begin{definition}
	A sequence of random variables $(X_0, X_1, \dots)$ is a Markov chain with state space $E$ and transition matrix $P$ if for all $t \geq 0$, 
	$$
	\P\big( X_{t+1} = y \, \big| \, X_0= x_0, \dots, X_t = x_t ) = P(x_t,y)
	$$
	for all $x_0, \dots, x_t$ such that $\P(X_0 = x_0, \dots, X_t = x_t) >0$.
\end{definition}

The transition matrix $P$ verifies therefore, for all $x \in E$,
\begin{equation}
	\sum_{y \in E} P(x,y) = 1.
\end{equation}

In order to simplify the notations, we will assume that $E = \{1,2, \dots, n\}$ and write for all $i,j \in E$, $P_{i,j} = P(j,i)$. 
\textbf{Note that we switched here the order of $i$ and $j$. This is not what is usually done in the literature, but this will allow us to be more coherent}.
Such matrix is said to be stochastic:

\begin{definition}[Stochastic matrix]
	A matrix $P \in \R^{n \times n}$ is said to be \emph{stochastic} if:
	\begin{enumerate}[label=(\roman*),noitemsep]
		\item $P_{i,j} \geq 0$ for all $1 \leq i,j \leq n$.
		\item $\sum\limits_{i=1}^n P_{i,j} = 1$, for all $1 \leq j \leq n$.
	\end{enumerate}
\end{definition}

Let $(X_0, X_1, \dots)$ be a Markov chain on $\{1, \dots, n\}$ with transition matrix $P$. For $t \geq 0$ we will encode the distribution of $X_t$ in the $1 \times n$ vector
$$
x^{(t)} = (x^{(t)}_1, \dots x^{(t)}_n) 
= \big(\P(X_t = 1), \dots, \P(X_t = n)\big) \in \Delta_n
$$
where $\Delta_n$ is the ``$n$-simplex''
$$
\Delta_n \defeq \Big\{ x \in \R^n \, \Big| \, \sum_{i=1}^n x_i = 1 \ \text{and} \ x_i \geq 0 \ \text{for all} \ i \Big\}.
$$


\begin{proposition}
	For all $t \geq 0$
	$$
	x^{(t+1)} = P x^{(t)}
	\quad \text{and consequently,} \quad
	x^{(t)} =  P^t x^{(0)}.
	$$
\end{proposition}
\begin{proof} Let $i \in \{1,\dots, n\}$.
	\begin{align*}
		x^{(t+1)}_i
		= \P(X_{t+1}=i)
		= \sum_{j=1}^n \P(X_{t+1}=i|X_t = j) \P(X_t = j)
		= \sum_{i=1}^n P_{i,j} x^{(t)}_j
		= (x^{(t)} P)_i.
	\end{align*}
\end{proof}

\begin{corollary}\label{cor:stab}
	Let $P$ be a stochastic matrix. Then
	\begin{itemize}
		\item For all $x \in \Delta_n$, $Px \in \Delta_n$.
		\item For all $t \geq 1$, $P^t$ is stochastic.
	\end{itemize}
\end{corollary}


\subsection{Invariant measures and the Perron-Frobenius Theorem}
We will be interested in the distribution of $X_t$ for $t$ large, that is the limit of $x^{(t)} = x^{(0)} P^t$. As we will see, under suitable conditions on the matrix $A$, this

\begin{definition}
	A vector $\mu \in \Delta_n$ is an invariant measure for the transition matrix $P$ if $\mu = P \mu$, i.e.\
	$$
	\text{for all} \ j \in \{1, \dots, n\}, \ \mu_i = \sum_{j=1}^n P_{i,j} \mu_j.
	$$
\end{definition}

\begin{remark}
	An invariant measure is an eigenvector of $P$ with associated eigenvalue $1$.
\end{remark}


\begin{theorem}[Perron-Frobenius, stochastic case]\label{th:perron_frobenius}
	Let $P$ be a stochastic matrix such that there exists $k \geq 1$ such that all the entries of $P^k$ are strictly positive. Then the following holds:
	\begin{enumerate}[label=(\roman*),noitemsep]
		\item\label{item:i} $1$ is an eigenvalue of $P$ and there exists an eigenvector $\mu \in \Delta_n$ associated to $1$.
		\item\label{item:ii} The eigenvectors associated to $1$ are unique up to scalar multiple (i.e.\ $\Ker(P-\Id) = \Span(\mu)$).
		\item\label{item:iii} For all $x \in \Delta_n$, $P^t x \xrightarrow[t \to \infty]{} \mu$.
	\end{enumerate}
\end{theorem}

Theorem \ref{th:perron_frobenius} is proved in the next section.

\begin{corollary}
	Let $P$ be a stochastic matrix such that there exists $k \geq 1$ such that all the entries of $P^k$ are strictly positive. Then there exists a unique invariant measure $\mu$ and for all initial condition $x^{(0)} \in \Delta_n$,
	$$
	x^{(t)} \xrightarrow[t \to \infty]{} \mu.
	$$
\end{corollary}

\subsection{Proof of Theorem \ref{th:perron_frobenius}}
We first prove the theorem in the case $k=1$, when $P_{i,j} > 0$ for all $i,j$.
\begin{lemma}\label{lem:contract}
	The mapping 
	$$
	\begin{array}{cccc}
		\varphi:& \Delta_n &\to& \Delta_n \\
				& x & \mapsto & Px
	\end{array}
	$$
	is contracting for the $\ell_1$-norm: there exists $c \in (0,1)$ such that for all $x,y \in \Delta_n$:
	$$
	\| Px - Py \|_1 \leq c \| x-y\|_1.
	$$
\end{lemma}
\begin{proof}
	%Let $x \in \Delta_n$. Since the entries of $x$ and $P$ are non-negative, the entries of $Px$ are also non-negative. Compute
	%$$
	%\|Px\|_1 = \sum_{i=1}^n (Px)_i = \sum_{i=1}^n \sum_{j=1}^n P_{i,j} x_j
	%= \sum_{j=1}^n \Big(\sum_{i=1}^n P_{i,j}\Big) x_j.
	%$$
	%Since $P$ is stochastic  we have $\sum_{i=1}^n P_{i,j} = 1$ which gives
	%$$
	%\|Px\|_1 = \sum_{j=1}^n x_j = 1.
	%$$
	%Hence $Px \in \Delta_n$. 
	First notice that $\varphi$ is well-defined by Corollary \ref{cor:stab}.
	Let us write $\alpha \defeq \min_{i,j} P_{i,j} \in (0,1)$.
	Let $x,y \in \Delta_n$. We will show that $\| Px - Py \|_1 \leq (1-\alpha) \| x-y\|_1$, i.e.\ $\|P z\|_1 \leq \alpha \|z\|_1$ where $z = x-y$. Compute
	$$
	\| P z\|_1 
	= \sum_{i=1}^n \big| (Pz)_i \big|
	= \sum_{i=1}^n \Big| \sum_{j=1}^n P_{i,j}z_j \Big|.
	$$
	Since $\sum_{j} z_j = 0$ we have $\sum_j (P_{i,j} - \alpha/n) z_j = \sum_j P_{i,j} z_j$. Hence
	$$
	\| P z\|_1 
	= \sum_{i=1}^n \Big| \sum_{j=1}^n (P_{i,j} - \alpha/n) z_j \Big|
	\leq \sum_{i=1}^n \sum_{j=1}^n (P_{i,j} - \alpha/n) |z_j| 
	= \sum_{j=1}^n (1-\alpha) |z_j|
	= (1-\alpha) \|z\|_1.
	$$
\end{proof}

Using Lemma \ref{lem:contract}, Banach fixed point Theorem tells us that $\varphi$ admits a unique fixed point $\mu$ on $\Delta_n$ (i.e.\ a unique $\mu \in \Delta_n$ such that $P\mu = \mu$) and that for all $x \in \Delta_n$, $P^t x \xrightarrow[t \to \infty]{} \mu$. This proves Theorem \ref{th:perron_frobenius} in the case $k=1$.
\\

In the case $k > 1$ we simply apply the result for $k=1$ to $P^k$.

This gives that there exists a unique $\mu \in \Delta_n$ such that $P^k \mu = \mu$. Multiplying by $P$ on both sides leads to $P^k (P\mu) = P\mu$. 
Since $P\mu \in \Delta_n$ we obtain that $P\mu = \mu$ by uniqueness of $\mu$. This proves \ref{item:i}. To prove \ref{item:ii} we consider $x \in \R^n$ such that $P x = x$. By iteration we get $P^k x = x$ which implies (using the result on $P^k$) that $x \in \Vect(\mu)$.
To prove \ref{item:iii} we fix $\ell \in \{0, \dots, k-1\}$. Let $x \in \Delta_n$. By applying the point \ref{item:iii} to $P^k$, we have 
$$
P^{kt} P^{\ell} x \xrightarrow[t \to \infty]{} \mu.
$$
Since this holds for all $\ell \leq k-1$ we obtain that $P^T x \xrightarrow[T \to \infty]{} \mu$ using the Euclidean division of $T$ by $k$.

\section{Example: Google's PageRank algorithm}


\vspace{1cm}
\centerline{\pgfornament[width=7cm]{71}}

%\bibliographystyle{plain}
%\bibliography{./references.bib}
\end{document}
