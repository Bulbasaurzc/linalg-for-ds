\documentclass[11pt,nocut]{article}

\usepackage{../latex_style/packages}
\usepackage{../latex_style/notations}
%\externaldocument{../lecture_02/lecture_02}
\externaldocument{../lecture_07/lecture_07}


\title{\vspace{-2.0cm}%
	Optimization and Computational Linear Algebra for Data Science\\
Lecture 12: Gradient descent}
\author{LÃ©o \textsc{Miolane} \ $\cdot$ \ \texttt{leo.miolane@gmail.com}}
\date{\today}

\begin{document}
\maketitle
\input{../preamble.tex}

\begin{center}
In these notes, $f$ denotes a twice differentiable \textbf{convex} function from $\R^n$ to $\R$.
\end{center}

\section{Gradient descent}

Given an initial point $x^{(0)} \in \R^n$, the gradient descent algorithm follows the updates:
\begin{equation}\label{eq:gradient_step}
x^{(t+1)} = x^{(t)} - \alpha_t \nabla f(x^{(t)}),
\end{equation}
where the step-size $\alpha_t$ remains to be determined.
The step \eqref{eq:gradient_step} is a very natural strategy to minimize $f$, since $-\nabla f(x)$ is the direction of steepest descent at $x$. Since $f(x+h) = f(x) + \langle \nabla f(x), h \rangle + o(\|h\|)$ we have
\begin{align*}
f(x^{(t+1)}) 
&= f(x^{(t)}) - \alpha_t \| \nabla f(x^{(t)}) \|^2 + o(\alpha_t) \\
&< f(x^{(t)}) 
\end{align*}
for $\alpha_t$ small enough (provided that $\nabla f(x^{(t)}) \neq 0$).
Hence is the step-sizes $\alpha_t$ are chosen very small, the sequence $(f(x^{(t)}))_{k \geq 0}$ is decreasing!
However, if $\alpha_t$ are too small, the algorithm may never converge.

\subsection{Convergence analysis}

\textbf{Notation}: Given a symmetric matrix $M$ we will denote by $\lambda_{\rm min}(M)$ and $\lambda_{\rm max}(M)$ the smallest and largest eigenvalues of $M$. 
\begin{definition}
	For $L,\gamma >0$, we say that a twice-differentiable convex function $f: \R^n \to \R$ is
	\begin{itemize}
		\item $L$-smooth if for all $x \in \R^n$, $\lambda_{\rm max}(H_f(x)) \leq L$.
		\item $\gamma$-strongly convex if for all $x \in \R^n$, $\lambda_{\rm min}(H_f(x)) \geq \gamma$.
	\end{itemize}
\end{definition}


\begin{theorem}
	Assume that $f$ is $L$-smooth and that $f$ admits a (global) minimizer $x^* \in \R^n$. Then
	the gradient descent iterates \eqref{eq:gradient_step} with constant step-size $\alpha_k = 1/L$ verify
	$$
	f(x^{(t)}) - f(x^*) \leq \frac{2 L \| x^{(0)} - x^* \|^2}{t+4}.
	$$
\end{theorem}

\paragraph{Why did we used step sizes of $1/L$ ?} If $f$ is $L$-smooth one can prove (see Homework~9) that for all $x,h \in \R^n$:
\begin{equation}\label{eq:upperL}
f(x+h) \leq f(x) + \langle \nabla f(x) , h \rangle + \frac{L}{2} \|h\|^2.
\end{equation}
Then, one can check (exercise!) that when $x$ is fixed, the minimum of the right-hand side is minimum for $h = - \frac{1}{L} \nabla f(x)$.

\begin{theorem}\label{th:gradient2}
	Assume that $f$ is $L$-smooth and $\gamma$-strongly convex. 
	Then $f$ admits a unique minimizer global $x^*$ and
	the gradient descent iterates \eqref{eq:gradient_step} with constant step-size $\alpha_k = 1/L$ verify
	$$
	f(x^{(t)}) - f(x^*) \leq \Big(1-\frac{\gamma}{L}\Big)^t (f(x^{(0)}) - f(x^*)).
	$$
\end{theorem}
\begin{remark}
	The $\gamma$-strong convexity of $f$ implies that for all $x \in \R^n$,
	$$
	\frac{\gamma}{2} \|x - x^* \|^2 \leq f(x)-f(x^*).
	$$
	Combining this with Theorem \ref{th:gradient2} gives a bound of the distance to the minimizer $x^*$:
	$$
	\|x^{(t)} - x^* \|^2 \leq \frac{2}{\gamma}\Big(1-\frac{\gamma}{L}\Big)^t (f(x^{(0)}) - f(x^*)).
	$$
\end{remark}

\begin{proof}
	Let $t \geq 0$. Applying \eqref{eq:upperL} for $x=x^{(t)}$ and $h=x^{(t)} - L^{-1} \nabla f(x^{(t)})$, we get
	$$
	f(x^{(t+1)}) 
	\leq f(x^{(t)}) - \frac{1}{L} \|\nabla f(x^{(t)}) \|^2 + \frac{1}{2L}\|\nabla f(x^{(t)}) \|^2
	= f(x^{(t)}) - \frac{1}{2L} \|\nabla f(x^{(t)}) \|^2.
	$$
Now, since $f$ is $\gamma$-strongly convex, we have for all $x \in \R^n$
$$
f(x) - f(x^*) \leq 2 \gamma \|\nabla f(x)\|^2.
$$
We get that $f(x^{(t+1)}) \leq f(x^{(t)}) - \frac{\gamma}{L}(f(x^{(t)}) - f(x^*))$, hence
$$
f(x^{(t+1)}) - f(x^*) \leq (1 - \frac{\gamma}{L})(f(x^{(t)}) - f(x^*)),
$$
from which the theorem follows.
\end{proof}

\subsection{Choosing the step size in practice}

In practice, one may not have access to $L$ and need hence to choose the step size $\alpha_t$. A popular method is the so-called ``backtracking line search'' a goes as follows.
Fix a parameter $\beta \in (0,1)$.
Start with $\alpha = 1$ and while
$$
f\big(x^{(t)} - \alpha \nabla f (x^{(t)}) \big) > f(x^{(t)}) - \frac{\alpha}{2} \|\nabla f(x^{(t)}) \|^2,
$$
update $\alpha =\beta \alpha$. Then choose $\alpha_t = \alpha$.

\section{Newton's method}

We assume here that $f$ is $\gamma$-strongly convex and $L$-smooth.
Newton's method performs updates according to
\begin{equation}\label{eq:newton}
	x^{(t+1)} = x^{(t)} - H_f(x^{(t)})^{-1} \nabla f(x^{(t)}).
\end{equation}
The (important!) difference with gradient descent is that the step-size $\alpha_k$ is now replaced by the inverse\footnote{The Hessian of $f$ if indeed invertible at all $x$ since its smallest eigenvalue is always greater than $\gamma >0$.} of the Hessian of $f$. The idea begin Newton's method is to minimize the second order approximation of $f$ at $x^{(t)}$ :
\begin{equation}\label{eq:sec}
	f(x^{(t)}+h) \simeq f(x^{(t)}) + \langle \nabla f(x^{(t)}), h \rangle + \frac{1}{2} h^{\sT} H_f(x^{(t)}) h
\end{equation}
with respect to $h$ and then choose $x^{(t+1)} = x^{(t)} + h$. It is an easy exercise to see that the minimizer of the right-hand side of \eqref{eq:sec} is $h=- H_f(x^{(t)})^{-1} \nabla f(x^{(t)})$, leading to the recursion \eqref{eq:newton}.
\\

It can be shown (see for instance \cite{boyd2004convex}) that for $t$ large enough
\begin{equation}\label{eq:newton}
\|x^{(t)} - x^* \|^2 \leq C e^{-\rho 2^t},
\end{equation}
where $C,\rho$ are constants depending on $f$ and $x^{(0)}$. We say that Newton's method converges \emph{quadratically} to the minimizer $x^*$. Newton's method is much faster than gradient descent, whose speed (given by Theorem \ref{th:gradient2}) is of order $C' e^{-\rho' t}$.
\\

The main drawback of Newton's method is its computational complexity. Each step of the method require to compute the inverse of the $n \times n$ Hessian matrix of $f$ at $x^{(t)}$, which require $O(n^3)$ operations. This makes Newton's method unpractical for large scale applications.


\section*{Further reading}


\vspace{1cm}
\centerline{\pgfornament[width=7cm]{71}}


\bibliographystyle{plain}
\bibliography{../references.bib}
\end{document}
